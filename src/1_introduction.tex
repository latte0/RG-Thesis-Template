\chapter{序論}
\label{introduction}

本章ではまず本研究を取り巻く背景について述べる。そして本研究の解決する課題及び課題を解決する意義、解決するための手法を提示する。
次に本論文がもたらした貢献と、解決した課題を述べ、最後に本論文の構成を概観する。

\section{はじめに}
\label{introduction:background}

\subsubsection{背景}


近年、画像認識や音声認識といった分野で機械学習の中でもディープラーニングと呼ばれる分野が急速発展し、自動運転やスマートスピーカーといったプロダクトとして人々の日常の中にも応用され始めている。
機械学習はプログラミングを容易にするPytorch~\cite{pytorch}やTensorflow~\cite{tensorflow}と呼ばれるライブラリの開発も積極的に行われているだけではなく
株式会社ソニーのNeural Network Console~\cite{sony}を筆頭とした、非エンジニアにも使いやすいGUIベースのツールも多数開発され続けている。
これらは機械学習におけるニューラルネットワークの構築を容易にするだけではなく、学習アルゴリズム自体も抽象化され複雑な式を理解せずとも使用できるようになっている。
また、ニューラルネットワークを構築する重要な要素の一つに活性化関数がある。
この活性化関数には、SigmoidやReLU~\cite{ReLU}などの一般的に用いられてきた。
そしてこれらは問題やデータに応じて最適な活性化関数を経験則に基づいて調整していた。
また、ニューラルネットワークでは特に出力層のアウトプットを考慮した活性化関数の組み合わせを採択することが多く、例えばResnet50~\cite{resnet50}では、
問題が分類ということを考慮され、中間層は全てReLU、出力層はSigmoidなどといった組み合わせが使用されている。

一方、統計学の分野では、リンク関数が未知の場合には、ノンパラメトリックな手法に基づきモデルを推論する幾つかの手法が考案されている。
カーネル密度推定を用いてノンパラメトリックに推定するという手法がIchimura(1993)~\cite{ichimura}によって提案されている。

\subsubsection{課題・手法}

ニューラルネットワーク構築の際の課題として、問題に応じた最適な活性化関数が未だわかっていないことが挙げられる。
全ての課題に同一の活性化関数を流用するのではなく、各課題においてその都度適切な活性化関数を導くことができればより良い精度が出せることが予想される。
特に出力層に用いる活性化関数はデータセットや問題の種類を意識する必要があり、精度に非常に直結するだけでなく、初学者にとっての構築を難しくする。
そこで本論文は事前に関数の形を指定しないカーネル密度推定を用いた活性化関数を提案する。
本論文ではカーネル密度推定とトレーニングデータの少数の点を用いて活性化関数の推論をするアルゴリズムを構築する。
提唱されたIchimura(1993)の方法では、トレーニングデータの全てを活性化関数の計算に使用する必要があったが、
ディープラーニング等のデータ量が非常に大きい場合での応用を考え計算時間を現実的なものにするため、使用するデータ点を可変にすることが可能なアルゴリズムを提案した。


\subsubsection{貢献}
\label{kouken}

以降本研究で提案するカーネル密度推定を用いた活性化関数をK-AFと表記する。
K-AFは実際のデータセットを用いて、ニューラルネットワークの出力層を本論文で提案する方法に置き換えることにより従来の活性化関数と同等かそれ以上の精度で予測できること示した。
本研究における主な貢献を以下にまとめる。

\begin{itemize}
  \item カーネル密度推定を用いた汎用的な活性化関数で実用的なものを完成させた。
  \item K-AFはさまざまなデータセットにおいて既存の活性化関数によりより良い精度を出すことを達成した。
  \item K-AFはいくつかのデータセットでは高い学習率でも安定した学習精度を出すことに成功した。
  \item K-AFを用いることによりデータセットに応じて出力層の活性化関数の形は従来のものではないことを示した。
  \item K-AFが勾配爆発(\ref{bakuhatsu}節 参照)しないための条件を探求した。
\end{itemize}

以上の貢献により以下の一般的な課題を解決した。

\begin{itemize}
  \item 出力層に用いる活性化関数を状況に応じた適切な形に変わる汎用的な関数を導として、高い精度を出せるようにした。
  \item 各問題やデータに応じた最適な活性化関数の形が模索できるようになった。
  \item 分類や回帰といった問題を考えずとも、高い精度でニューラルネットワークが構築できるようになった。
\end{itemize}



\section{本論文の構成}

本論文における以降の構成は次の通りである．

~\ref{background}章では、ノンパラメトリックモデル、カーネル密度推定などといった本研究へとつながる背景の解説し、これらの手法における課題を洗い出す。
~\ref{proposed}章では、本研究におけるカーネル密度推定を用いた活性化関数についての解説を行い、提案手法の解説の詳細を述べる。
~\ref{implementation}章では、~\ref{proposed}章で述べた手法の実装及び、実装における留意点を述べる。
~\ref{evaluation}章では、\ref{background}章で求められた課題に対しての評価を行い、考察する。
~\ref{conclusion}章では、実験の結果に対する考察を行い,本研究を行う上で浮上した提案手法の限界を示し,今後の研究方針についてまとめる。






%%% Local Variables:
%%% mode: japanese-latex
%%% TeX-master: "../thesis"
%%% End:
