\chapter{提案手法}
\label{proposed}



\begin{figure}[hbtp]
\includegraphics[width=15cm]{asset/proposed_method.png}
	\caption{提案手法}
	\label{proposed_method}
\end{figure}


第\ref{background}章では本論文の提案手法を理解するための知識と、その背景から現状の課題点について述べた。

本章では、背景を整理しつつ\ref{kadai}節で述べた課題を解決する本論文の提案手法の位置付けについて述べる。
\ref{history_activation}節では既存の活性化関数の動向を踏まえながら本提案手法へとつながる過程を述べる。
\ref{position_proposed}節では本研究の研究的な位置付けについて解説する。
\ref{math}節ではK-AFの数式について解説し、活性化関数として使用できることを示す。
\ref{al}節ではK-AFのアルゴリズムについて概説し、実装の理解を深める。




\section{活性化関数と背景}
\label {history_activation}


\begin{figure}[hbtp]
\includegraphics[width=15cm]{asset/history_af.png}
	\caption{活性化関数の歴史}
	\label{history_af}
\end{figure}

現在、深層学習に利用できる活性化関数が研究されている。特に近年では、中間層にReLUを用い出力層にSigmoid関数を用いた組み合わせがよく用いられている。
しかし、これらの組み合わせは経験的なものだけでなく、データに対する人間の知識が事前に必要とされる。

また、本研究で提案する活性化関数はSwishやMishなどからの単調増加性の仮定を外した点に着目した。
Sigmoidはロジスティック回帰から生まれたものであり、ReLU自体も実験的に精度が良いと導出されただけのものであった。
それらの単調増加性という性質は機械学習の観点では本来必要ではないと考えることもできる。
図\ref{history_af}のように、活性化関数はSwishやMishなどといった形へと進化する中で、単調増加性を仮定する必要がなくなった。
isotonic regression同様に単調増加性について仮定しているようであるが、本研究からはその仮定を外す。
そうすることで、より精度の高い結果を導き出すことができると予想される。
関数の推定はカーネル密度推定で行う。
これにより活性化関数を既存の関数から選択するのではなく、関数空間全体から活性化関数を推定することができる。
これにより、これまでディープラーニングで課題とされてきた活性化関数の選択の問題を解決することが可能となり、新たなアプローチが可能になることが予想される。
また、これまで選択してきた活性化関数が実験的に正しいかどうか判定することも可能であると考えられる。


\section{提案手法の位置付け}
\label {position_proposed}

本研究の提案手法の位置付けをまとめた図を\ref{proposed_method}に示した。
統計学の世界でSingleIndexModelにおけるIchimura(1993)の手法を多次元化し、学習アルゴリズムにPAVアルゴリズムを組み合わせた手法をニューラルネットワークの活性化関数に応用する。



\section{K-AF}
\label {math}
本論文で私が提案する活性化関数を数式\ref{eq:k-af}で表現する。

\begin{eqnarray}
G(\mathbf{X}_iw, \mathbf{X}^{calc}, \mathbf{Y}^{calc})=\frac{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j w - \mathbf{X}_i w}{h_{calc}}\right)\mathbf{Y}^{calc}_j}{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j w - \mathbf{X}_i w}{h_{calc}}\right)}
\label{eq:k-af}
\end{eqnarray}


$ \mathbf{X}^{calc}_j $及び$ \mathbf{Y}^{calc}_j $ は計算用に用いるデータ点である。
現在の機械学習における問題の多くは学習に用いるデータセットが非常に大きい。
そのため、Ichimura(1993)~\cite{ichimura}ではデータセットの数だけで表現していたが、一部を省略することにより少ないデータ点で表現する。
少ないデータ点で記述することは、活性化関数の形の単純化と計算量を減らすことにも直結する。

詳細な数式の導出はappendix\ref{appendix:calc}で述べた。


\subsection{バンド幅推定}


\begin{figure}[hbtp]
    \begin{center}
            \begin{minipage}{0.40\hsize}
                \includegraphics[width=5cm]{asset/k_af_band_big.png}
                    \caption{バンド幅が大きいと、K-AFで表現できる活性化関数の数は減る。}
                    \label{k_af_band_big}
            \end{minipage}
            \begin{minipage}{0.40\hsize}
            \hspace{10pt}
                \includegraphics[width=5cm]{asset/k_af_band_small.png}
                    \caption{バンド幅が小さいと、K-AFで表現できる活性化関数の形は大きくなる。}
                    \label{k_af_band_small}
            \end{minipage}
    \end{center}
\end{figure}


カーネル密度推定は関数の形状を推定するにあたってバンド幅の大きさが非常に重要になる。
バンド幅は大きいほど関数の自由度が減り、小さいほど自由度が大きくなることが容易に推定できる。

図\ref{k_af_band_small}の方が表現の幅が広いが、勾配爆発の問題や過学習の問題が考えらる。
本実験ではこのバンド幅も学習のパラメータに含めることで最適なバンド幅を推定できるようにした。

\section{アルゴリズム}
\label{al}
K-AFのアルゴリズムの概要を{\bf Algorithm}\ref{alg:fixed-u-alg}に記述した。

入力の次元を$ d $　出力の次元を$ e $とする。$ m $をミニバッチのサイズ、$ w^t $を$t$ステップ目の重みのパラメータとする。
また、$x$のデータセットの集合を$ \mathcal{D}_x $　$y$のデータセットの集合を$ \mathcal{D}_y $、それらから$n$個サンプリングすることを$ \mathbf{Y}^{calc} \sim_n \mathcal{D}_y $と記述する。
$ \mathrm{E} $を目的関数とした時、以下のアルゴリズムで最適化を表現することができる。



\begin{algorithm}[]
	\caption{\KAF}
	\label{alg:fixed-u-alg}
\begin{algorithmic}
	\STATE {\bfseries Input:} data $\langle (\mathbf{X}_i, y_i) \rangle_{i=1}^m \in
	\reals^d \times \reals^e$, $G: ( \reals^d, \reals^n, \reals^n ) \rightarrow  \reals^e$.
	\STATE $\mathbf{X}^{calc} \sim_n \mathcal{D}(\mathbf{X})$;
    \STATE $\mathbf{Y}^{calc} \sim_n \mathcal{D}(\mathbf{Y})$;
	\FOR {$t = 1, 2, \ldots$}
	\STATE $g^t(x) := G(\mathbf{X} \cdot  \mathbf{W}^t, \mathbf{X}^{calc}, \mathbf{Y}^{calc} )$;
	\STATE $ \displaystyle{\min_{w} \mathrm{E}(w)} = \frac{1}{2}\sum_m (y_i - g^t(\mathbf{X}_i))^2 $
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{itemize}
  \item データセット$\mathcal{D}_x$,$\mathcal{D}_y$から任意個数の$ \mathbf{X}^{calc} $と$ \mathbf{Y}^{calc} $から取り出す。
  \item 現状のパラメータ$ w $, $\mathcal{D}_x$, $\mathcal{D}_y$を用いて、リンク関数$ g^t $の計算を行う。
  \item そのリンク関数を用いて$ y $の値との最小二乗誤差を取り$ w $に対して最適化を行う。
\end{itemize}


pythonでの実装についてはappendix\ref{appendix:algorithm}に記述した。






%%% Local Variables:
%%% mode: japanese-latex
%%% TeX-master: "../bthesis"
%%% End:
