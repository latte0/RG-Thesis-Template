\chapter{実装}
\label{implementation}

\ref{honkadai}節を踏まえつつ\ref{proposed}節で解決に近づけるために、本章では3つの実験とその設定内容について述べる。

本章では本研究における実装環境,提案手法の実装,提案手法の評価に用いるデータセットについて述べる.
\ref{impl_env}節では本研究における実験のための実行環境及び事前知識について述べる。
\ref{exp_common}節では、K-AFの性能を最も引き出す可能性がある学習の設定を調査する。
実験1 (\ref{exp1}節)ではK-AFの性能を既存の活性化関数と比較する実験を行う。
また、関数の形状を学習すると同時にどのように損失関数が小さくなるかValidationLossのグラフを取得する。
実験2 (\ref{exp2}節)では各データセットにおいての活性化関数の形を調査する実験を行う。
実験3 (\ref{exp3}節)では、K-AFの性能を最も引き出す可能性がある、学習の設定を調査する。

これら3つの実験を踏まえて、K-AFの課題に対しての実用性だけでなく、K-AFの性質についても調査し、今後の発展のための鍵となる結論へとつなげる。



\section{実装環境}
\label{impl_env}



本研究において利用した実装環境を 表\ref{impl_table} に記す。
また、本手法を動かすために使用したライブラリのバージョンを表\ref{impl2_table}に記す。
PyTorch~\cite{pytorch}, Chainer~\cite{chainer},  Tensorflow~\cite{tensorflow} は計算グラフの自動微分ライブラリであり、深層ニューラルネットワークの研究や開発にも用いられる。
その中でもPytorchは実装コストが低く研究領域に適用しやすいことから、本研究で用いた。


\begin{table}[htbp]
    \begin{center}
        \caption{本研究の実行環境}
        \label{impl_table}
        \begin{tabular}{|c|c|}
        \hline
        項目              & 仕様 \\
        \hline
        CPU               &  2.6GHz Intel core i7 \\
        \hline
        メモリ             & 16GB 2400 MHz DDR4 \\
        \hline
        ストレージ          & Macintosh HD 256GB \\
        \hline
        OS               & macOS High Sierra 10.13.6 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}


\begin{table}[htbp]
    \begin{center}
        \caption{本研究のに使用したライブラリのバージョン}
        \label{impl2_table}
        \begin{tabular}{|c|c|}
        \hline
        環境              & バージョン \\
        \hline
        Python            & 3.8.5  \\
        \hline
        scikit-learn      & 0.23.2\\
        \hline
        numpy             & 1.18.5 \\
        \hline
        pytorch           & 2.5.0 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}

\section{本実験での共通項目}
\label{exp_common}

実験に用いる活性化関数の形を図\ref{k-af-net}に示す。
\begin{figure}[hbtp]
    \begin{center}
        \includegraphics[width=10cm]{asset/k-af-net.png}
            \caption{実験で用いるニューラルネットワークの概要図}
            \label{k-af-net}
    \end{center}
\end{figure}

今回の実験では、簡易的なモデルで活性化関数の性能を試していく。
中間層の次元数を$ l_1 $とし、\textcircled{\scriptsize 1}にはReLUを用いる。\textcircled{\scriptsize 2}の部分を可変的にさまざまな活性化関数へと変えていく。
本論文で提案する、K-AFは\textcircled{\scriptsize 2}の部分において性能を評価する。

比較用の活性化関数には以下の表\ref{list:af_table}に記述してあるものを用いる。


\begin{table}[htbp]
    \begin{center}
        \caption{本実験で使用する活性化関数リスト}
        \label{list:af_table}
        \vspace{2mm} 
        \begin{tabular}{ |c| }
        \hline
        ReLU \\
        Sigmoid \\
        Tanh   \\
        Mish  \\
        Swish  \\
        K-AF(本手法)   \\
        \hline
        \end{tabular}
    \end{center}
\end{table}



活性化関数の性能の比較実験のために、以下の表項目\ref{list:learning_algorithm_change}を変えながら実験する。
データセットにはsckit-learn~\cite{scikit-learn}のライブラリのデータセットに対してデフォルトで入ってるものを想定する。
各種データセットの詳細については\url{https://scikit-learn.org/stable/datasets/toy_dataset.html}こちらのページが参考になる。



\begin{table}[htbp]
    \begin{center}
        \caption{学習においての使用するニューラルネットワークの学習アルゴリズム}
        \label{list:learning_algorithm_change}
        \vspace{2mm} 
        \begin{tabular}{ |c|c| }
        \hline
        学習においての変更項目 & 変更のリスト\\
        \hline
        LearningRate           & [  $10^{-1}$,  $10^{-2}$,  $10^{-3}$,  $10^{-4}$, $10^{-5}$, $10^{-6}$ ,$10^{-7}$]    \\
        \hline
        Initializer         & [ Xavier, KaimingUniform]   \\
        \hline
        Regularizer           & [ 何もなし, L1ノルム, L2ノルム]     \\
        \hline
        Optimizer         & [ SGD, RMSProp, AdaGrad, Adam]   \\
        \hline
        データセット &  [ iris, digits, wine, boston, breast\_cancer ]    \\
        \hline
        \end{tabular}
    \end{center}
\end{table}



\subsection{比較データ}

他の活性化関数と適当に比較するために、以下の条件を比較して実験を行う。


\begin{table}[htbp]
    \begin{center}
        \caption{実験1に用いるデータセットの情報}
        \label{dataset_name}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c|c|c|c| }
        データセット名 & サンプル数 & 入力の次元 & 出力の次元 & 出力の形式 & 中間層の数 $ {l_1} $ \\
        \hline
        iris           & 150    & 3         & 3        & 分類      & 4 \\
        digits         & 1797   & 3         & 64       & 分類      & 100 \\
        wine           & 178    & 3         & 13       & 分類      & 40 \\
        boston         & 506    & 13        & 1        & 回帰      & 20 \\
        breast\_cancer & 569    & 30        & 2        & 分類      & 30 \\
        \end{tabular}
    \end{center}
\end{table}


\subsection{実装における留意点}
実験のために必要なハイパーパラメータとして以下のパラメータを実験前に設定することとした。

\begin{table}[htbp]
    \begin{center}
        \caption{本実験において使用するハイパーパラメータ}
        \vspace{2mm} 
        \scalebox{0.7}[0.7]{
            \begin{tabular}{||c | c |c||}
            ハイパーパラメータ & 記号 & 説明 \\
            \hline
            epoch数                           & epoch\_num      & 学習の速度、最終的なValidationLossを確認するために十分なepoch数を取る。  \\
            実験回数                           & exp\_num     & 数回実験を行なった平均をとり、結果を保証する。 \\
            カーネル密度推定に用いるデータ数        & calc\_num           & $ \mathbf{X}^{calc} $ に用いるデータ数を表す。  \\
            \end{tabular}
        }
    \end{center}
\end{table}

また本論文の実験では学習の手法として、オフライン学習（バッチ学習）により全ての実験を行なった。そのためepoch数は学習の際にループで回した回数に相当する。



\section{実験1 既存の活性化関数との比較実験}
\label{exp1}
\subsection{実装手法}

表\ref{list:learning_algorithm_change}に記述した5つのデータセットを軸に\ref{list:af_table}の活性化関数での比較実験を
LearningRate、Optimizer、Initializer、Regularizerを変更しながら実験した。
K-AFの性能が有利にならないように、可能な限り任意の組み合わせで実験を行い性能の評価を行う。



\subsection{評価手法}

回帰問題と分類問題におけるれぞれの評価時における精度指標（Metrics）として、
回帰問題では平均二乗誤差（MSE）を、分類問題では正解率（Accuracy）を用いる。
これらは、最も基礎的で一般的な評価関数である。

また本実験ではValidationDataはTrainingDataと等しいものとして実験を進める。


\subsubsection{MSEの式}


評価に用いる推論したデータを$ \hat{y}_i $、正解のラベル$ y_i^{true} \sim \mathcal{D}_y $とした時

\begin{eqnarray}
\mathbf{MSE} = \frac{\sum_n \| \hat{y}_i - y_i^{true}\|}{n}
\label{eq:accuracy}
\end{eqnarray}

で計算することとする。


\subsubsection{Accuracyの式}

評価に用いる推論したデータを$ \hat{y}_i $と正解のラベル$ y_i^{true} \sim \mathcal{D}_y $とした時

\begin{eqnarray}
\mathbf{Acc} = \frac{\sum_n \| \hat{y}_i == y_i^{true}\|^2}{n}
\label{eq:accuracy}
\end{eqnarray}

ここで、$ \| \hat{y}_i == y_i^{true}\| $とは$ \hat{y}_i $と $ y_i^{true} $が等しければ$ 1 $、等しくなければ $ 0 $を表現する式とする。

\subsection{各種データセットでの比較実験の詳細}

各種データセットではAccuracyによる性能の比較だけでなくValidationLossのグラフから学習速度を評価する。
また、step数が$0$に近い場合活性化関数によってはかなり大きな値が出力されてしまいグラフ全体が可視化に際に評価しにくくなるので、$100$から表示することとする。

\subsubsection{irisでの比較実験}
\label{impl:iris}

irisはアヤメの分類問題であり、統計学の分野で最も一般的に性能評価がおなわれるものである。
optimzerにSGDを用いRegularizerを特に使わない理由はirisは単純なデータセットのため、収束することを前提に考えているからである。
実験に用いる設定と環境を表\ref{exp:iris}に示した。


\begin{table}[htbp]
\label{exp:iris}
    \begin{center}
        \caption{irisでの実験と設定}
        \label{exp:iris}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        設定名 & 設定1 & 設定2 \\
        \hline
        LearningRate         & $ 10^{-1} $ & $ 10^{-2} $ \\
        \hline
        Initializer       & KaimingUniform &  KaimingUniform \\
        \hline
        Optimizer           & SGD & SGD \\
        \hline
        Regularizer     & なし & なし \\
        \hline
        epoch\_num       & 1000 &  1000 \\
        \hline
        exp\_num         & 3 & 3 \\
        \hline
        calc\_num        & 30 & 30 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}



\subsubsection{digitsでの比較実験}
\label{impl:digits}

digitsは数字の画像の分類問題であり、機械学習の分野で一般的に性能評価の対象として使用されるものである。
digitsは次元数が高いデータセットであるため、OptimizerにはAdamとSGD, レギュライザーにはL1ノルムとL2ノルムを使用した。
実験に用いる設定と環境を表\ref{exp:digits}に示した。

\begin{table}[htbp]
    \begin{center}
        \caption{digitsでの実験と設定}
        \label{exp:digits}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        設定名 & 設定1 & 設定2 \\
        \hline
        LearningRate         & $ 10^{-2} $ & $ 10^{-3} $ \\
        \hline
        Initializer       & KaimingUniform &  KaimingUniform \\
        \hline
        Optimizer           & SGD & SGD \\
        \hline
        Regularizer     & なし & なし \\
        \hline
        epoch\_num       & 2000 &  10000 \\
        \hline
        exp\_num         & 3 & 3 \\
        \hline
        calc\_num        & 36 & 36 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}



\subsubsection{wineでの比較実験}
\label{impl:wine}

wineとはワインの種類の判別を13個の入力次元の性質を用いて3つにカテゴライズするデータセットである。
主に決定木の性能評価に使われることが多い。本実験では以下のデータセットを用いて行う。
実験に用いる設定と環境を表\ref{exp:wine}に示した。


\begin{table}[htbp]
    \begin{center}
        \caption{wineでの実験と設定}
        \label{exp:wine}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        設定名 & 設定1 & 設定2 \\
        \hline
        LearningRate         & $ 10^{-3} $ & $ 10^{-4} $ \\
        \hline
        Initializer       & KaimingUniform & KaimingUniform \\
        \hline
        Optimizer           & SGD & Adam \\
        \hline
        Regularizer     & なし & なし \\
        \hline
        epoch\_num       & 3000 &  3000 \\
        \hline
        exp\_num         & 3 & 3 \\
        \hline
        calc\_num        & 27 & 36 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}


\subsubsection{bostonでの比較実験}
\label{impl:boston}

bostonはボストンの住宅価格を回帰で推定する問題である。K-AFが回帰問題に対しても有効であることを示すため、この実験を行う。
実験に用いる設定と環境を表\ref{exp:boston}に示した。

\begin{table}[htbp]
    \begin{center}
        \caption{bostonでの実験と設定}
        \label{exp:boston}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        設定名 & 設定1 & 設定2 \\
        \hline
        LearningRate         & $ 10^{-5} $ & $ 10^{-5} $ \\
        \hline
        Initializer       & KaimingUniform  & Xavier \\
        \hline
        Optimizer           & SGD & Adam \\
        \hline
        Regularizer     & なし & なし \\
        \hline
        epoch\_num       & 1000 &  1000 \\
        \hline
        exp\_num         & 5 & 5 \\
        \hline
        calc\_num        & 26 & 26 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}

また、bostonの実験ではK-AFが勾配爆発することがあったが、今回は性能評価のため、勾配爆発しなかった場合でMSEを求めた。


\subsubsection{breast\_cancerでの比較実験}
\label{impl:breastcancer}

breast\_cancerは乳がんのデータセットである。実験に用いる設定と環境を表\ref{exp:breastcancer}に示した。

\begin{table}[htbp]
    \begin{center}
        \caption{breast\_cancerでの実験と設定}
        \label{exp:breastcancer}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        設定名 & 設定1 & 設定2 \\
        \hline
        LearningRate         & $ 10^{-3} $ & $ 10^{-3} $ \\
        \hline
        Initializer       & KaimingUniform & KaimingUniform \\
        \hline
        Optimizer           & SGD & SGD \\
        \hline
        Regularizer     & なし & なし \\
        \hline
        epoch\_num       & 1000 &  1000 \\
        \hline
        exp\_num         & 5 & 5 \\
        \hline
        calc\_num        & 29 & 285 \\
        \hline
        \end{tabular}
    \end{center}
\end{table}

また、breast\_cancerの実験ではK-AFが勾配爆発することがあったが、データセットによる問題であることに気づいたので、そのような場合も含めたAccuracyを結果に記した。

\vspace{-5mm} 

\section{実験2 K-AFの関数形状の調査及び損失関数の詳細調査}
\label{exp2}

2つ目の実験では推論した活性化関数の形を表\ref{dataset_name2}に記載したデータセット観測し、既存の活性化関数との違いを定性評価を行う。
活性化関数の種類を分割すると以下のようになっている。
また、ニューラルネットワークの設定は表\ref{exp1}の設定で各データセットごとに二つずつ結果を取得する。

\begin{table}[htbp]
    \begin{center}
        \caption{活性化関数の関数的な意味}
        \label{af-class}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c| }
        \hline
        単調増加関数か & 上限値があるか   \\
        \hline
        ○ or × & ○ or ×  \\
        \hline
        \end{tabular}
    \end{center}
\end{table}


表\ref{af-class}を軸に、K-AFの形状を分析し、既存の活性化関数と比べて何が違うか評価を行う。
表\ref{af-class}の「単調増加関数か」というのは、SigmoidやReLUでは表現の幅として不十分であることを示す指標となる。
「上限値があるか」というのはSigmoidやTanhといった関数では表現できないことを示す指標になる。これは一般的に回帰問題はSigmoidやTanhで不十分であることを意味する。


またそのような活性化関数を計算するためのニューラルネットワークの設定を以下の表に示した設定で行う。
\begin{table}[htbp]
    \begin{center}
        \caption{実験2に用いるデータセットの情報及びLearningRateの設定}
        \label{dataset_name2}
        \vspace{2mm} 

        \scalebox{0.7}[0.7]{
            \begin{tabular}{ |c|c|c|c|c|c|c|c| }
            \hline
            データセット名 & LearningRate & Optimizer & Initializer & Reguralizer & 中間層の次元数 & calc\_num & epoch\_num \\
            \hline
            iris           & $ 10^{-1} $    & SGD         & KaimingUniform        & なし      & 4 & 30 & 1000 \\
            \hline
            digits         & $ 10^{-1} $    & SGD         & KaimingUniform       & なし      & 100 & 36 & 1000 \\
            \hline
            wine           & $ 10^{-2} $    & SGD         & KaimingUniform       & なし      & 40 & 36 & 1000 \\
            \hline
            boston         & $ 10^{-5} $    & SGD        & KaimingUniform        & なし      & 20 & 26 & 1000 \\
            \hline
            breast\_cancer & $ 10^{-2} $    & SGD        & KaimingUniform        & なし      & 30 & 285 & 1000 \\
            \hline
            \end{tabular}
        }
    \end{center}
\end{table}

素早く形状の形の変化を録画する予定のため、学習率は高めに設定した。
epochが1000の地点での活性化関数の形状を記録することとする。
また、本実験は勾配爆発が起きていない場合の活性化関数の形状を記録し、評価する。



\section{実験3 K-AFの性能が上がる条件探査}
\label{exp3}

K-AFは実験の中で勾配爆発問題が発生することが発覚した。
特にbostonや breast\_cancer では適当なパラメータで学習を開始すると、学習が勾配爆発する可能性が高いことが実験の中から判明した。
breast\_cancerの場合に実際に勾配爆発したことは本論文でも明らかになっている。
また、勾配の爆発問題はパラメータの勾配を上限値でクリッピングすることである程度減らすことが知られている。
本実験では、二つの実験を行う。

\subsection{実験3.1 ニューラルネットワークの設定変更により精度向上の条件調査}
\label{exp3.1}

\begin{table}[htbp]
    \begin{center}
        \caption{実験3.1において使用するニューラルネットワークの構成の組み合わせ}
        \label{list:learning_algorithm_change3}
        \vspace{2mm} 
        \begin{tabular}{ |c|c| }
        \hline
        ニューラルネットワークの構成 & 変更のリスト \\
        \hline
        Initializer         & [ Xavier, KaimingUniform]   \\
        \hline
        Regularizer           & [ 何もなし, L1ノルム, L2ノルム]     \\
        \hline
        Optimizer         & [ SGD, RMSProp, AdaGrad, Adam]   \\
        \hline
        \end{tabular}
    \end{center}
\end{table}


K-AFが一般的にどのようなニューラルネットワークの設定の場合に性能を出しやすいかという問題を、表\ref{list:learning_algorithm_change3}での全てのパターンの組み合わせで実験することで、調査する。
本実験ではデータセットとしてbostonを用いて性能評価を行う。

また、本実験に必要なパラメータを以下の表\ref{exp31hyper}に設定する。


\begin{table}[hbtp]
    \begin{center}
        \caption{実験3で使うハイパーパラメータ}
        \label{exp31hyper}
        \vspace{2mm} 
        \scalebox{1.0}[1.0]{
            \begin{tabular}{||c|c|c||}
            \hline
            ハイパーパラメータ  & 値 \\
            \hline
            LearningRate & $ 10^{-5} $ \\
            \hline
            中間層の次元数 $ l_1 $ & 20  \\
            \hline
            epoch\_num & 5  \\
            \hline
            exp\_num & 1000 \\
            \hline
            calc\_num & 50  \\
            \hline
            \end{tabular}
        }
    \end{center}
\end{table}

epoch\_numが少ない理由は勾配爆発するときは初期のタイミングで勾配爆発するからである。
1000回実験する中で勾配爆発頻度の回数を記録する実験をこなう。
そして、各ニューラルネットの構成で記録しどの組み合わせが一番勾配爆発の回数が少ないか調査する。
また、実験3.1ではLearningRateは小さい方が勾配爆発の確率が減ることは自明なためLearningRateは$ 10^{-5} $で固定するものとする。
また、calc\_numが大きい場合も勾配爆発の確率が低いことが\ref{result:breastcancer}でも明らかになるため、今回は$ 50 $で固定をする。



\subsection{実験3.2 クリッピングを用いた勾配爆発の制御}
\label{exp3.2}
実験3.2では表\ref{exp:boston}の設定1の条件に対してクリッピングを用いる場合とそうでない場合に勾配爆発の回数がどのように変化するか実験をする。
また学習性能についての評価を行うため、勾配爆発が発生しなかった場合のMSEを用いた比較を行う。
これにより、クリッピングがK-AFの学習に対してどの程度有効か議論する。
本実験では、式\ref{eq:hassan}の上限値$ M $を0.025で設定する。
また、ハイパーパラメータの設定は表\ref{exp31hyper}を用いる。


%%% Local Variables:
%%% mode: japanese-latex
%%% TeX-master: "../bthesis"
%%% End:
