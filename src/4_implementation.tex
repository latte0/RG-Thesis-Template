\chapter{実装}
\label{implementation}

本章では本研究における実装環境,提案手法の実装,提案手法の評価に用いるデータセットについて述べる.
\ref{impl_env}では本研究における実験のための実行環境及び事前知識について述べる。
\ref{exp1}ではK-AFの性能を既存の活性化関数と比較する実験を行う。
\ref{exp2}では各データセットにおいての活性化関数の形を調査する実験をこなう。
\ref{exp3}では、K-AFの性能を最も引き出す可能性がある、学習の設定を調査する。



\section{実装環境}
\label{impl_env}



本研究において利用した実装環境を \ref{impl_table} に示す. 提案手法の実装は Pytorch 及
を用いた.  PyTorch~\cite{pytorch}, Chainer~\cite{chainer},  Tensorflow~\cite{tensorflow} は計算グラフの自動微分ライブラリであり, 深層ニューラルネットワークの研究や開発にも用いられる.
Pytorchを用いた理由は実装コストが低く研究領域に従事できるところにある。


\begin{table}[htbp]
\label{impl_table}
    \begin{center}
        \caption{本研究の実行環境}
        \vspace{5mm} 
        \begin{tabular}{l*{2}{c}r}
        ソフトウェア              & バージョン \\
        \hline
        Python            & 3.8.5  \\
        CPU               & Intel core i7  \\
        PyTorch           & 6 \\
        \end{tabular}
    \end{center}
\end{table}



\vspace{-15mm} 
\section{実験1}
\label{exp1}

実験に用いる活性化関数の形を図\ref{k-af-net}に示す。
\begin{figure}[hbtp]
\label{k-af-net}
    \begin{center}
        \includegraphics[width=10cm]{asset/k-af-net.png}
            \caption{実験で使うニューラルネットワークの概要図}
            \label{neural_network1}
    \end{center}
\end{figure}

今回の実験では、簡易的なモデルで活性化関数の性能を試していく。
中間層の数を$ l_1 $とし、\textcircled{\scriptsize 1}にはReLUを用いる。\textcircled{\scriptsize 2}の部分を可変的にさまざまな活性化関数へと変えていく。
本論文で提案する、K-AFは\textcircled{\scriptsize 2}の部分において性能を評価する。

比較用の活性化関数には以下を用いる。
\begin{itemize}
\label{list:active}
    \setlength{\parskip}{0cm} % 段落間
    \setlength{\itemsep}{0cm} % 項目間
    \item ReLU
    \item Sigmoid
    \item Linear
    \item TanH
    \item Mish
    \item Swish
    \item K-AF(本手法)
\end{itemize}


活性化関数の性能の比較実験のために、以下の項目を変えながら実験する。データセットにはsckit-learn~\cite{scikit-learn}のライブラリのデータセットに対してデフォルトで入ってるものを想定する。
各種データセットの詳細については\url{https://scikit-learn.org/stable/datasets/toy_dataset.html}こちらのページが参考になる。

\begin{itemize}
\label{exp_list}
    \setlength{\parskip}{0cm} % 段落間
    \setlength{\itemsep}{0cm} % 項目間
    \item ラーニングレート(lr) [ $10^{-5}$, $10^{-6}$ ,$10^{-7}$]
    \item 初期値 [ Xavier, kaiming uniform]
    \item レギュラライザー [ 何もなし, L1ノルム, L2ノルム]
    \item optimizer [ SGD, Momentum, AdaGrad, Adam]
    \item データセット [ iris, digits, wine, boston, linnerud]
\end{itemize}



\subsection{比較データ}

他の活性化関数と適当に比較するために、以下の条件を比較して実験を行う。


\begin{table}[htbp]
    \begin{center}
        \caption{実験のデータセットの名称}
        \vspace{5mm} 
        \begin{tabular}{ |c|c|c|c|c| }
        データセット名 & 入力の次元 & 出力の次元 & 出力の形式 & 中間層の数$ {l_1} $\\
        \hline
        iris         & 3         & 3        & 分類      & 10 \\
        digits       & 3         & 10       & 分類      & 10 \\
        wine        & 3         & 13       & 分類      & 10 \\
        boston       & 3         & 6        & 回帰      & 10 \\
        linnerud     & 3         & 3        & 回帰      & 10 \\
        \end{tabular}
    \end{center}
\end{table}


\section{実装における留意点}
実験のために必要なハイパーパラメータとして以下のパラメータを実験前に設定することとした。

\begin{table}[htbp]
    \begin{center}
        \caption{実験のデータセットの名称}
        \vspace{5mm} 
        \scalebox{0.7}[0.7]{
            \begin{tabular}{||c | c |c||}
            ハイパーパラメータ & 記号 & 説明 \\
            \hline
            epoch数                           & epoch       & 学習の速度、最終的なval-Lossを見るのに十分なepoch数を取る。  \\
            実験回数                           & exp\_num     & 数回実験を行なった平均をとり、結果を保証する。 \\
            カーネル密度推定に使うデータ数        & n           & $ X^{calc} $ に使うデータ数を表す。  \\
            \end{tabular}
        }
    \end{center}
\end{table}



\section{実験1}

\subsection{実装手法}

\ref{exp_list}に記述した５つのデータセットを軸に\ref{list:active}の活性化関数での比較実験を
ニューラルネットワーク、中間層、学習率 、勾配アルゴリズム、イニシャライザ、ステップ数 、ノーマライザー
を変更しながら実験した。


\section{活性化関数}




\section{実験2}
\label{exp2}
２つ目の実験では推論した活性化関数の形を観測し、既存の活性化関数との違いを定性評価を行う
\subsection{比較データ}


\section{実験3}
\label{exp3}
K-AFにおける学習の難点である、勾配消失について定量評価を行う。
\subsection{比較データ}


%%% Local Variables:
%%% mode: japanese-latex
%%% TeX-master: "../bthesis"
%%% End:
