Abstract of Bachelor's Thesis - Academic Year 2020
\begin{center}
\begin{large}
\begin{tabular}{|p{0.97\linewidth}|}
    \hline
      \etitle \\
    \hline
\end{tabular}
\end{large}
\end{center}

~ \\


The activation function in neural networks is one of the important mathematical elements to expand the range of functions that neural networks can represent.
Functions such as Sigmoid and ReLU are commonly used as its activation function, and have dramatically improved the performance of machine learning.
Research to investigate better activation functions is still ongoing, and new methods such as Swish (2017) and Mish (2019) are being found vigorously.
However, the question of what to use for the activation function in order to create a more accurate model remains unsolved, as it has only been determined empirically.
In this paper, we applied the semiparametric model in the world of statistics to derive a method for learning while inferring the appropriate activation function for the model.
By doing so, we were able to produce highly accurate models without being aware of the problem we were dealing with.
We also succeeded in deriving higher accuracy than the existing activation functions in some patterns.
In addition, such functions can now be explored from the entire function, and we can determine whether the activation functions we have been using have sufficient representation for the model. These results show that it is possible to create and train models with a high degree of completeness without being aware of the shape of the training data, such as classification and regression.
We hope that this research will lead to the improvement of the versatility of neural networks, and contribute to a society where machine learning can be used more widely with people.



~ \\
Keywords : \\
\underline{1. Deep lerning},
\underline{2. Activation Function},
\underline{3. Non parametric},
\underline{4. Kernel Function}
\begin{flushright}
\edept \\
\eauthor
\end{flushright}
