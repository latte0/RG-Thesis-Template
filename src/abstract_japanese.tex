卒業論文要旨 - 2020年度 (令和02年度)
\begin{center}
\begin{large}
\begin{tabular}{|M{0.97\linewidth}|}
    \hline
      \title \\
    \hline
\end{tabular}
\end{large}
\end{center}

~ \\



ニューラルネットワークにおける活性化関数とは、ニューラルネットワークの表現できる関数の幅を広げるための重要な数学的要素の一つである。
その活性関数にはSigmoidやReLUといった関数が一般的に用いられ、機械学習の性能を飛躍的に向上させてきた。
より良い活性化関数を調査する研究は現在も行われており、Swish(2017)やMish(2019)といった新しい手法も精力的に見つけられている。
しかしながら、より精度が高いモデルを作るために、活性化関数に何を用いるかという問題は経験的に判断されているだけで未解決のままである。
本論文では統計の世界におけるセミパラメトリックモデルを応用した手法を用いて、モデルに応じた適切な活性化関数を推論しながら学習する手法を導いた。
それにより、扱っている問題を意識することなく高い精度のモデルを制作することができるようになった。
また、いくつかのパターンで既存の活性化関数よりも高い精度を導くことに成功した。
また、そのような関数が関数全体から探査できるようになり、今まで用いてきた活性化関数がそのモデルに対して十分な表現を持っているかどうか判断できるようになった。このような成果により、分類や回帰といった学習データの形状を意識せずとも高い完成度でモデルの作成から学習まで行えることを示した。
本研究が、ニューラルネットワークの汎用度の向上に繋がり、機械学習がより幅広く人々と関わっていく社会への貢献へとつながることを望む。


~ \\
キーワード:\\
\underline{1. ディープラーニング},
\underline{2. 活性化関数},
\underline{3. ノンパラメトリック},
\underline{4. カーネル密度推定}
\begin{flushright}
\dept \\
\author
\end{flushright}
