\appendix
\chapter{ガウス分布とカーネル密度推定}


\section{K-AFの導出}
\label{appendix:calc}

本研究で実際に使用したアルゴリズムに用いた式実際に導出する。
Ichimura(1993)~\cite{ichimura}の手法を用いてまずは以下の式に変換する。


\begin{eqnarray}
G(\mathbf{X}_i, \mathbf{W})=\frac{\sum_{i\neq j} K\left(\frac{\mathbf{X}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h}\right)\mathbf{Y}_j}{\sum_{i\neq j} K\left(\frac{\mathbf{X}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h}\right)}
\label{calc:k-af}
\end{eqnarray}

ここで、$ \sum_{i\neq j} K\left(\frac{\mathbf{X}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h}\right) \approx	\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h_{calc}}\right)$
となるようなバンド幅$ h_{calc} $を見つけることで、全てのデータ点を使わなくとも$ \mathbf{X}_{calc} $を用いて\ref{calc:k-af}を近似することができる。

これにより\ref{calc:k-af}は以下の式に直すことができる。

\begin{eqnarray}
G(\mathbf{X}_i, \mathbf{W}) \approx \tilde{G}(\mathbf{X}_i, \mathbf{W}, \mathbf{X}^{calc}, \mathbf{Y}^{calc})) = \frac{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h_{calc}}\right)\mathbf{Y}^{calc}_j}{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j \mathbf{W} - \mathbf{X}_i \mathbf{W}}{h_{calc}}\right)}
\label{calc:k-af-2}
\end{eqnarray}

以上により、Ichimura(1993)の手法に対してデータ点を減らしても近似できることを示した。


\chapter{カーネル活性化関数の実装}
\label{appendix:algorithm}


\section{クラス}


中間層が一つのK-AFの計算を考慮した実装クラスを\ref{python_impl}に示す。

実装の全容は\href{https://github.com/latte0/graduation\_thesis}{https://github.com/latte0/graduation\_thesis}に公開している。

\begin{lstlisting}[caption=Pytorchを用いたK-AFの計算用のクラス,label=python_impl]
class Net(nn.Module):

    def __init__(self, Y, calc_Y, X, calc_X, settings):
        super(Net, self).__init__()

        self.fc1 = nn.Linear(DATA_INPUT_LENGTH, DATA_MID_LENGTH, bias=False)
        self.fc2 = nn.Linear(DATA_MID_LENGTH, DATA_OUTPUT_LENGTH, bias=False)
        # leave_ont_outのために事前に入力と出力をセットしておく
        self.Y = Y
        self.calc_Y = calc_Y
        self.calc = False
        # バンド幅も推定する
        self.h = nn.Parameter(torch.tensor(1.5, requires_grad=True))

        self.last_layer_result = []
        self.sigmoid = nn.Sigmoid()

    # kernel推定量の計算
    def kernel(self, Zw):
        numerator = 0
        denominator = 0
        result = []
        for j, X_j in enumerate(self.train_X):

            Xw = self.fc2(F.relu(self.fc1(X_j)))
            tmp = gauss((Xw - Zw) / self.h)

            tmp[j] = 0
            denominator += tmp
            numerator += tmp * self.Y[j]

        g = numerator/denominator
        return g


    def forward(self, x):

        xw = F.relu(self.fc1(x))
        xw = self.fc2(xw)

        y = self.kernel(xw)

        return y


\end{lstlisting}


\chapter{カーネル活性化関数の学習過程の動画}
\label{appendix:movie}

活性化関数の学習が進んでいく様子を録画し、githubに公開している。



\begin{itemize}
  \item 実験2(\ref{evo2:iris_result}節)のirisのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/iris.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/iris.mov?raw=true}
  \item 実験2(\ref{evo2:digits_result}節)のdigitsのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/digits.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/digits.mov?raw=true}
  \item 実験2(\ref{evo2:wine_result}節)のwineのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/wine.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/wine.mov?raw=true}
  \item 実験2(\ref{evo2:boston_result}節)のbostonのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/boston.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/boston.mov?raw=true}
\end{itemize}



\chapter{bostonの勾配爆発回数の記録}
\label{appendix:error}

実験3.1\ref{exp3.1}の表\ref{bostonbest}を導くために必要な情報をまとめた。
各活性化関数の設定においての勾配爆発の頻度を記した結果を以下の表\ref{appendix:errorcount_table}に記述する。

\begin{table}[htbp]
    \begin{center}
        \caption{bostonを推論した時の勾配爆発の頻度と性能。LearningRateは $ 10^{-5} $で固定する。 }
        \label{appendix:errorcount_table}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Initializer & Optimizer &  Reguralizer & 発散回数 & 発散確率 \\
        \hline
        KaimingUniform & Adagrad & l1 & 622 & 62.2\% \\
        \hline
        KaimingUniform & Adagrad & l2 & 613 & 61.3\% \\
        \hline
        KaimingUniform & Adagrad & なし & 635 & 63.5\% \\
        \hline
        KaimingUniform & Adam & l1 & 632 & 63.2\% \\
        \hline
        KaimingUniform & Adam & l2 & 615 & 61.5\% \\
        \hline
        KaimingUniform & Adam & なし & 628 & 62.8\% \\
        \hline
        KaimingUniform & RMSprop & l1 & 639 & 63.9\% \\
        \hline
        KaimingUniform & RMSprop & l2 & 622 & 62.2\% \\
        \hline
        KaimingUniform & RMSprop & なし & 629 & 62.9\% \\
        \hline
        KaimingUniform & SGD & l2 & 585 & 58.5\% \\
        \hline
        KaimingUniform & SGD & l1 & 619 & 61.9\% \\
        \hline
        KaimingUniform & SGD & なし & 601 & 60.1\% \\
        \hline
        Xavier & Adagrad & l1 & 312 & 31.2\% \\
        \hline
        Xavier & Adagrad & l2 & 321 & 32.1\% \\
        \hline
        Xavier & Adagrad & なし & 314 & 31.4\% \\
        \hline
        Xavier & Adam & l1 & 290 & 29.0\% \\
        \hline
        Xavier & Adam & l2 & 294 & 29.4\% \\
        \hline
        Xavier & Adam & なし & 275 & 27.5\% \\
        \hline
        Xavier & RMSprop & l1 & 298 & 29.8\% \\
        \hline
        Xavier & RMSprop & l2 & 313 & 31.3\% \\
        \hline
        Xavier & RMSprop & なし & 328 & 32.8\% \\
        \hline
        Xavier & SGD & l1 & 294 & 29.4\% \\
        \hline
        Xavier & SGD & l2 & 309 & 30.9\% \\
        \hline
        Xavier & SGD & なし & 321 & 32.1\% \\
        \hline
        \end{tabular}
    \end{center}
\end{table}
