\appendix
\chapter{ガウス分布とカーネル密度推定}


\section{K-AFの導出}
\label{appendix:calc}

本研究で実際に使用したアルゴリズムに用いた数式実際に導出する。
Ichimura(1993)~\cite{ichimura}の手法を用いてまずは以下の式に変換する。


\begin{eqnarray}
G(\mathbf{X}_iw)=\frac{\sum_{i\neq j} K\left(\frac{\mathbf{X}_j w - \mathbf{X}_i w}{h}\right)Y_j}{\sum_{i\neq j} K\left(\frac{\mathbf{X}_j w - \mathbf{X}_i w}{h}\right)}
\label{calc:k-af}
\end{eqnarray}

ここで、$ \sum_{i\neq j} K\left(\frac{\mathbf{X}_j w - \mathbf{X}_i w}{h}\right) \approx	\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j w - \mathbf{X}_i w}{h_{calc}}\right)$
となるようなバンド幅$ h_{calc} $を見つけることで、全てのデータ点を使わなくとも$ \mathbf{X}_{calc} $を用いて\ref{calc:k-af}を近似することができる。

これにより\ref{calc:k-af}は以下の式に直すことができる。

\begin{eqnarray}
G(\mathbf{X}_iw) \approx \frac{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j w - \mathbf{X}_i w}{h_{calc}}\right)\mathbf{Y}^{calc}_j}{\sum_{i\neq j} K\left(\frac{\mathbf{X}^{calc}_j w - \mathbf{X}_i w}{h_{calc}}\right)}
\label{calc:k-af-2}
\end{eqnarray}

以上により、Ichimura(1993)raの手法に対してデータ点を減らしても近似できることを示した。


\chapter{カーネル活性化関数の実装}
\label{appendix:algorithm}


\section{クラス}


中間層が一つのK-AFの計算を考慮した実装クラスを\ref{python_impl}に示す。

実装の全ては \href{https://github.com/latte0/graduation\_thesis}{https://github.com/latte0/graduation\_thesis}. に公開している。

\begin{lstlisting}[caption=Pytorchを用いたK-AFの計算用のクラス,label=python_impl]
class Net(nn.Module):

    def __init__(self, Y, calc_Y, X, calc_X, settings):
        super(Net, self).__init__()

        self.fc1 = nn.Linear(DATA_INPUT_LENGTH, DATA_MID_LENGTH, bias=False)
        self.fc2 = nn.Linear(DATA_MID_LENGTH, DATA_OUTPUT_LENGTH, bias=False)
        # leave_ont_outのために事前に入力と出力をセットしておく
        self.Y = Y
        self.calc_Y = calc_Y
        self.calc = False
        # バンド幅も推定する
        self.h = nn.Parameter(torch.tensor(1.5, requires_grad=True))

        self.last_layer_result = []
        self.sigmoid = nn.Sigmoid()

    # kernel推定量の計算
    def kernel(self, Zw):
        numerator = 0
        denominator = 0
        result = []
        for j, X_j in enumerate(self.train_X):

            Xw = self.fc2(F.relu(self.fc1(X_j)))
            tmp = gauss((Xw - Zw) / self.h)

            tmp[j] = 0
            denominator += tmp
            numerator += tmp * self.Y[j]

        g = numerator/denominator
        return g


    def forward(self, x):

        xw = F.relu(self.fc1(x))
        xw = self.fc2(xw)

        y = self.kernel(xw)

        return y


\end{lstlisting}


\chapter{カーネル活性化関数の学習過程の動画}
\label{appendix:movie}

活性化関数の学習が進んでいく様子を録画し、githubに公開している。



\begin{itemize}
  \item 実験2\ref{exp2}のirisのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/iris.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/iris.mov?raw=true}
  \item 実験2\ref{exp2}のdigitsのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/digits.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/digits.mov?raw=true}
  \item 実験2\ref{exp2}のwineのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/wine.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/wine.mov?raw=true}
  \item 実験2\ref{exp2}のdostonのK-AFの学習過程 \\ \href{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/boston.mov?raw=true}{https://github.com/latte0/RG-Thesis-Template/blob/main/asset/boston.mov?raw=true}
\end{itemize}



\chapter{bostonエラー回数の記録}
\label{appendix:error}

各活性化関数の設定においてのエラーの頻度を以下の表に記述する。

\begin{table}[htbp]
    \begin{center}
        \caption{bostonを推論した時のエラーの頻度と性能。LearningRateは $ 10^{-5} $ }
        \label{appendix:errorcount_table}
        \vspace{2mm} 
        \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Initializer & Optimizer &  Reguralizer & 発散回数 & 発散確率 \\
        \hline
        kaiming\_uniform & Adagrad & l1 & 622 & 62.2\% \\
        \hline
        kaiming\_uniform & Adagrad & l2 & 613 & 61.3\% \\
        \hline
        kaiming\_uniform & Adagrad & non & 635 & 63.5\% \\
        \hline
        kaiming\_uniform & Adam & l1 & 632 & 63.2\% \\
        \hline
        kaiming\_uniform & Adam & l2 & 615 & 61.5\% \\
        \hline
        kaiming\_uniform & Adam & non & 628 & 62.8\% \\
        \hline
        kaiming\_uniform & RMSprop & l1 & 639 & 63.9\% \\
        \hline
        kaiming\_uniform & RMSprop & l2 & 622 & 62.2\% \\
        \hline
        kaiming\_uniform & RMSprop & non & 629 & 62.9\% \\
        \hline
        kaiming\_uniform & SGD & l2 & 585 & 58.5\% \\
        \hline
        kaiming\_uniform & SGD & l1 & 619 & 61.9\% \\
        \hline
        kaiming\_uniform & SGD & non & 601 & 60.1\% \\
        \hline
        Xavier & Adagrad & l1 & 312 & 31.2\% \\
        \hline
        Xavier & Adagrad & l2 & 321 & 32.1\% \\
        \hline
        Xavier & Adagrad & non & 314 & 31.4\% \\
        \hline
        Xavier & Adam & l1 & 290 & 29.0\% \\
        \hline
        Xavier & Adam & l2 & 294 & 29.4\% \\
        \hline
        Xavier & Adam & non & 275 & 27.5\% \\
        \hline
        Xavier & RMSprop & l1 & 298 & 29.8\% \\
        \hline
        Xavier & RMSprop & l2 & 313 & 31.3\% \\
        \hline
        Xavier & RMSprop & non & 328 & 32.8\% \\
        \hline
        Xavier & SGD & l1 & 294 & 29.4\% \\
        \hline
        Xavier & SGD & l2 & 309 & 30.9\% \\
        \hline
        Xavier & SGD & non & 321 & 32.1\% \\
        \hline
        \end{tabular}
    \end{center}
\end{table}
