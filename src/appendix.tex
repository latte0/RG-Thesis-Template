\appendix
\chapter{ガウス分布とカーネル密度推定}


\section{K-AFの導出}
\label{appendix:calc}

本研究で実際に使用したアルゴリズムに用いた数式実際に導出する。
Ichimura(1993)~\cite{ichimura}の手法を用いてまずは以下の式に変換する。


\begin{eqnarray}
G(X_iw)=\frac{\sum_{i\neq j} K\left(\frac{X_j w - X_i w}{h}\right)Y_j}{\sum_{i\neq j} K\left(\frac{X_j w - X_i w}{h}\right)}
\label{calc:k-af}
\end{eqnarray}

ここで、$ \sum_{i\neq j} K\left(\frac{X_j w - X_i w}{h}\right) \approx	\sum_{i\neq j} K\left(\frac{X^{calc}_j w - X_i w}{h_{calc}}\right)$
となるようなバンド幅$ h_{calc} $を見つけることで、全てのデータ点を使わなくとも$ X_{calc} $を用いて\ref{calc:k-af}を近似することができる。

これにより\ref{calc:k-af}は以下の式に直すことができる。

\begin{eqnarray}
G(X_iw) \approx \frac{\sum_{i\neq j} K\left(\frac{X^{calc}_j w - X_i w}{h_{calc}}\right)Y^{calc}_j}{\sum_{i\neq j} K\left(\frac{X^{calc}_j w - X_i w}{h_{calc}}\right)}
\label{calc:k-af-2}
\end{eqnarray}

以上により、Ichimura(1993)raの手法に対してデータ点を減らしても近似できることを示した。


\chapter{カーネル活性化関数の実装}
\label{appendix:algorithm}


\section{クラス}


中間層が一つのK-AFの計算を考慮した実装クラスを\ref{python_impl}に示す。

実装の全ては \href{https://github.com/latte0/graduation\_thesis}{graduation\_thesis}. に公開している。

\begin{lstlisting}[caption=Pytorchを用いたK-AFの計算用のクラス,label=python_impl]
class Net(nn.Module):

    def __init__(self, Y, calc_Y, X, calc_X, settings):
        super(Net, self).__init__()

        self.fc1 = nn.Linear(DATA_INPUT_LENGTH, DATA_MID_LENGTH, bias=False)
        self.fc2 = nn.Linear(DATA_MID_LENGTH, DATA_OUTPUT_LENGTH, bias=False)
        # leave_ont_outのために事前に入力と出力をセットしておく
        self.Y = Y
        self.calc_Y = calc_Y
        self.calc = False
        # バンド幅も推定する
        self.h = nn.Parameter(torch.tensor(1.5, requires_grad=True))
        self.h_middle = torch.tensor(1.0)

        self.last_layer_result = []
        self.sigmoid = nn.Sigmoid()

    # kernel推定量の計算
    def kernel(self, Zw):
        numerator = 0
        denominator = 0
        result = []
        for j, x_j in enumerate(self.train_X):

            Xw = self.fc2(F.relu(self.fc1(x_j)))
            tmp = gauss((Xw - Zw) / self.h)

            tmp[j] = 0
            denominator += tmp
            numerator += tmp * self.Y[j]

        g = numerator/denominator
        return g


    def forward(self, x):

        xw = F.relu(self.fc1(x))
        xw = self.fc2(xw)

        y = self.kernel(xw)

        return y


\end{lstlisting}

また、活性化関数の学習が進んでいく様子を録画し、youtubeに公開している。



\begin{itemize}
  \item \href{https://github.com/latte0/graduation\_thesis}{実験1のwineの学習サンプル}
  \item \href{https://github.com/latte0/graduation\_thesis}{実験1のwineの学習サンプル}
  \item \href{https://github.com/latte0/graduation\_thesis}{実験1のwineの学習サンプル}
  \item \href{https://github.com/latte0/graduation\_thesis}{実験1のwineの学習サンプル}
  \item \href{https://github.com/latte0/graduation\_thesis}{実験1のwineの学習サンプル}
\end{itemize}

